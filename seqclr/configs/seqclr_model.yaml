global: 
  name: seqclr
  phase: train
  stage: FINETUNE # ['en_SEQCLR', 'en_SEQCLR_HARD', 'FINETUNE', 'de_SEQCLR', 'de_SEQCLR_HARD']
  workdir: workdir
  seed: ~

training:
  epochs: 20
  train_bs: 8 # 32
  eval_bs: 2 # 8
  save_iters: 1000
  eval_iters: 1000
  logg_iters: 1000
  warmup_steps: 1000
  save_total_limit: 3

optimizer:
  wd: 0.001
  lr: 0.0003
  lr_scheduler_type: cosine
  
wandb:
  report_to: wandb
  project_name: seqclr-UA # seqclr, seqclr-UA
  run_name: FINETUNE_large-960h-lv60

seqclr:
  speech_backbone: large-960h-lv60 # base, small, medium, large
  outdir: '../checkpoints/FINETUNE_large-960h-lv60'
  dataset: {
    train_mode: train, 
    test_mode: test,
  }
  proj: {
    layer: backbone_feature,  # 'feature'|'backbone_feature'
    scheme: bilstm,  # null|'bilstm'|'linear_per_column'|'attn_linear_per_column'
  }
  instance_mapping: {
    frame_to_instance: True, # Ture|False
    fixed: instances,  # instances|frames
    w: 150, # It works when frame_to_instance is False.
  }

asr:
  seqclr_ckp: null
  freeze_feature_encoder: True # True|False
  outdir: '../checkpoints/FINETUNE_large-960h-lv60_fz'
  dataset: 
    type: ua # ua
    mode_ua: {
      train_mode: ['train'], 
      test_mode: ['test'],
    }