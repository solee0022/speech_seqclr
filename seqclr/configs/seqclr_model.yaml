global: 
  name: seqclr
  phase: train
  stage: en_SEQCLR
  workdir: workdir
  seed: ~

training:
  epochs: 100
  train_bs: 32
  eval_bs: 8
  save_iters: 500
  eval_iters: 500
  logg_iters: 500
  warmup_steps: 10
  save_total_limit: 3

optimizer:
  wd: 0.001
  lr: 0.0003
  lr_scheduler_type: cosine
  
wandb:
  report_to: wandb
  project_name: seqclr-UA # seqclr, seqclr-UA
  run_name: de_SEQCLR_large-v3

model:
  name: 'semimtr.modules.model_seqclr_vision.SeqCLRModel'
  speech_backbone: large-v3 # base, small, medium, large-v3
  backbone_freeze: False
  outdir: '../checkpoints/en_SEQCLR_large-v3'
  dataset: {
    train_mode: train, 
    test_mode: test,
  }
  proj: {
    layer: backbone_feature,  # 'feature'|'backbone_feature'
    scheme: bilstm,  # null|'bilstm'|'linear_per_column'|'attn_linear_per_column'
  }
  instance_mapping: {
    frame_to_instance: True, # Ture|False
    fixed: instances,  # instances|frames
    w: 150, # It works when frame_to_instance is False.
  }

decoder:
  seqclr_ckp: '../checkpoints/en_SEQCLR_large-v3'
  seqclr_freeze:  True # True|False
  pretrained_ckp: null
  outdir: '../checkpoints/de_SEQCLR_large-v3'
  dataset: 
    type: ua # ua
    mode_ua: {
      train_mode: ['train'], 
      test_mode: ['test'],
    }